{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Svh0HqBC3U1i"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5m9uQQA3pzi"},"outputs":[],"source":["\n","import os\n","os.listdir(\"/content/drive/MyDrive/AutoModelForSequenceClassification/\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERyBj-mv3qng"},"outputs":[],"source":["!pip install transformers datasets\n","!pip install torch"]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","from difflib import SequenceMatcher\n","\n","# ===============================\n","# ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù CSV\n","# ===============================\n","csv_path = \"/content/drive/MyDrive/AutoModelForSequenceClassification/updates.csv\"\n","\n","try:\n","    df = pd.read_csv(csv_path, encoding='utf-8')\n","    print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­\")\n","except UnicodeDecodeError:\n","    try:\n","        df = pd.read_csv(csv_path, encoding='utf-8-sig')\n","        print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­ (utf-8-sig)\")\n","    except:\n","        df = pd.read_csv(csv_path, encoding='cp1256')\n","        print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­ (cp1256)\")\n","except Exception as e:\n","    print(f\"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {str(e)}\")\n","    raise\n","\n","print(f\"\\nâœ… Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª: {len(df)}\")\n","print(f\"âœ… Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {len(df.columns)}\")\n","\n","# ===============================\n","# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n","# ===============================\n","def normalize_arabic(text):\n","    \"\"\"ØªÙ†Ø¸ÙŠÙ ÙˆØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\"\"\"\n","    if pd.isna(text):\n","        return \"\"\n","\n","    text = str(text).strip()\n","    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)\n","    text = re.sub(r'Ø©', 'Ù‡', text)\n","    text = re.sub(r'Ù‰', 'ÙŠ', text)\n","    text = re.sub(r'Ø¦', 'Ø¡', text)\n","    text = re.sub(r'[^\\w\\s]', ' ', text)\n","    text = ' '.join(text.split())\n","\n","    return text.lower()\n","\n","# ===============================\n","# Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© (Ù…Ø´ ÙƒÙ„ ÙƒÙ„Ù…Ø© Ù„ÙˆØ­Ø¯Ù‡Ø§!)\n","# ===============================\n","def phrase_search_score(search_phrase, target_text):\n","    \"\"\"\n","    Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© - Ø§Ù„Ø¬Ù…Ù„Ø© Ù„Ø§Ø²Ù… ØªÙƒÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯Ø© ÙƒÙ„Ù‡Ø§ Ù…Ø¹ Ø¨Ø¹Ø¶\n","\n","    Ù…Ø«Ø§Ù„:\n","    - search_phrase = \"Ø­Ù„ÙŠØ¨ ÙƒØ§Ù…Ù„ Ø§Ù„Ø¯Ø³Ù…\"\n","    - target_text = \"Ø­Ù„ÙŠØ¨ ÙƒØ§Ù…Ù„ Ø§Ù„Ø¯Ø³Ù… UHT Ù…Ø¹Ù‚Ù…\"\n","    - Ø§Ù„Ù†ØªÙŠØ¬Ø©: âœ… Ù…ÙˆØ¬ÙˆØ¯Ø© (Ø§Ù„Ø¬Ù…Ù„Ø© ÙƒÙ„Ù‡Ø§ Ù…ÙˆØ¬ÙˆØ¯Ø©)\n","\n","    - target_text = \"Ø­Ù„ÙŠØ¨ Ù†ØµÙ Ø¯Ø³Ù…\"\n","    - Ø§Ù„Ù†ØªÙŠØ¬Ø©: âŒ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯Ø© (Ù…Ø´ Ù†ÙØ³ Ø§Ù„Ø¬Ù…Ù„Ø©)\n","    \"\"\"\n","    if not search_phrase or not target_text:\n","        return 0.0\n","\n","    search_clean = normalize_arabic(search_phrase)\n","    target_clean = normalize_arabic(target_text)\n","\n","    # 1. Ø§Ù„Ø¬Ù…Ù„Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„Ø¶Ø¨Ø·\n","    if search_clean == target_clean:\n","        return 1.0\n","\n","    # 2. Ø§Ù„Ø¬Ù…Ù„Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙƒØ¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù†Øµ\n","    if search_clean in target_clean:\n","        # Ù†Ø­Ø³Ø¨ ÙƒØ§Ù… Ù†Ø³Ø¨Ø© Ø§Ù„Ø¬Ù…Ù„Ø© Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„ÙƒÙ„ÙŠ\n","        phrase_length = len(search_clean)\n","        target_length = len(target_clean)\n","        coverage = phrase_length / target_length\n","\n","        # ÙƒÙ„ Ù…Ø§ Ø§Ù„Ø¬Ù…Ù„Ø© ØªÙƒÙˆÙ† Ø¬Ø²Ø¡ Ø£ÙƒØ¨Ø± Ù…Ù† Ø§Ù„Ù†ØµØŒ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø£Ø¹Ù„Ù‰\n","        base_score = 0.85\n","        bonus = coverage * 0.15\n","        return min(base_score + bonus, 1.0)\n","\n","    # 3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¹Ø§Ù„ÙŠ Ù„Ù„Ø¬Ù…Ù„Ø©\n","    similarity = SequenceMatcher(None, search_clean, target_clean).ratio()\n","    if similarity >= 0.9:\n","        return similarity\n","\n","    # 4. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¬Ù…Ù„Ø© Ø¨ØªØ±ØªÙŠØ¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª (Ø­ØªÙ‰ Ù„Ùˆ ÙÙŠÙ‡ ÙƒÙ„Ù…Ø§Øª Ø²ÙŠØ§Ø¯Ø© Ø¨ÙŠÙ†Ù‡Ù…)\n","    search_words = search_clean.split()\n","    target_words = target_clean.split()\n","\n","    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªØ³Ù„Ø³Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª\n","    if len(search_words) > 1:\n","        # Ù†Ø´ÙˆÙ Ù„Ùˆ ÙƒÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨\n","        search_index = 0\n","        for target_word in target_words:\n","            if search_index < len(search_words):\n","                if search_words[search_index] in target_word or target_word in search_words[search_index]:\n","                    search_index += 1\n","\n","        # Ù„Ùˆ Ù„Ù‚ÙŠÙ†Ø§ ÙƒÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨\n","        if search_index == len(search_words):\n","            # Ù†Ø­Ø³Ø¨ Ø¯Ø±Ø¬Ø© Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¨ÙŠÙ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª\n","            match_ratio = len(search_words) / len(target_words)\n","            return 0.7 + (match_ratio * 0.2)\n","\n","    # 5. Ù„Ùˆ Ø§Ù„Ø¬Ù…Ù„Ø© Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯Ø© Ø®Ø§Ù„Øµ\n","    return 0.0\n","\n","# ===============================\n","# Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø³Ù‘Ù†\n","# ===============================\n","def enhanced_search(product_name=\"\", sector_name=\"\", min_confidence=0.7, top_n=30):\n","    \"\"\"\n","    Ø¨Ø­Ø« Ù…Ø­Ø³Ù‘Ù† - ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¬Ù…Ù„Ø© ÙƒØ§Ù…Ù„Ø© Ù…Ø´ ÙƒÙ„ ÙƒÙ„Ù…Ø© Ù„ÙˆØ­Ø¯Ù‡Ø§\n","\n","    Parameters:\n","    - product_name: Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬ (Ø¬Ù…Ù„Ø© ÙƒØ§Ù…Ù„Ø©)\n","    - sector_name: Ø§Ø³Ù… Ø§Ù„Ù‚Ø·Ø§Ø¹ (Ø¬Ù…Ù„Ø© ÙƒØ§Ù…Ù„Ø©)\n","    - min_confidence: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø«Ù‚Ø© (Ø§ÙØªØ±Ø§Ø¶ÙŠ 70%)\n","    - top_n: Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n","    \"\"\"\n","\n","    if not product_name and not sector_name:\n","        return {\n","            \"found\": False,\n","            \"message\": \"âš ï¸ ÙŠØ¬Ø¨ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬ Ø£Ùˆ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„\"\n","        }\n","\n","    results = []\n","\n","    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙƒÙ„ ØµÙ\n","    for index, row in df.iterrows():\n","        row_product = str(row.get('Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬', row.get('product_name', row.get('Ø§Ù„Ù…Ù†ØªØ¬', ''))))\n","        row_sector = str(row.get('Ø§Ù„Ù‚Ø·Ø§Ø¹', row.get('sector', row.get('Sector', ''))))\n","\n","        if row_product == \"nan\":\n","            row_product = \"\"\n","        if row_sector == \"nan\":\n","            row_sector = \"\"\n","\n","        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…Ù†ØªØ¬ (Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©)\n","        product_score = 0.0\n","        if product_name:\n","            product_score = phrase_search_score(product_name, row_product)\n","        else:\n","            product_score = 0.5\n","\n","        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù‚Ø·Ø§Ø¹ (Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©)\n","        sector_score = 0.0\n","        if sector_name:\n","            sector_score = phrase_search_score(sector_name, row_sector)\n","        else:\n","            sector_score = 0.5\n","\n","        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© (ÙˆØ²Ù† Ø£ÙƒØ¨Ø± Ù„Ù„Ù…Ù†ØªØ¬)\n","        total_confidence = (product_score * 0.7) + (sector_score * 0.3)\n","\n","        # ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø«Ù‚Ø©\n","        if total_confidence < min_confidence:\n","            continue\n","\n","        # Ø¬Ù…Ø¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª\n","        classifications = {}\n","        for col in df.columns:\n","            if col not in ['Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬', 'Ø§Ù„Ù‚Ø·Ø§Ø¹', 'product_name', 'sector', 'Sector', 'Ø§Ù„Ù…Ù†ØªØ¬']:\n","                value = str(row.get(col, \"\"))\n","                if value and value != \"nan\" and value.strip():\n","                    classifications[col] = value\n","\n","        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØªØ·Ø§Ø¨Ù‚\n","        match_details = []\n","        if product_name and product_score > 0.5:\n","            if product_score >= 0.95:\n","                match_details.append(\"âœ“ Ù…Ø·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ø§Ù‹\")\n","            elif product_score >= 0.85:\n","                match_details.append(\"âœ“ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù…Ù„Ø© ÙƒØ§Ù…Ù„Ø©\")\n","            elif product_score >= 0.7:\n","                match_details.append(\"ØªØ·Ø§Ø¨Ù‚ Ù‚ÙˆÙŠ Ù„Ù„Ø¬Ù…Ù„Ø©\")\n","            else:\n","                match_details.append(\"ØªØ·Ø§Ø¨Ù‚ Ø¬Ø²Ø¦ÙŠ\")\n","\n","        if sector_name and sector_score > 0.5:\n","            if sector_score >= 0.95:\n","                match_details.append(\"âœ“ Ù‚Ø·Ø§Ø¹ Ù…Ø·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ø§Ù‹\")\n","            elif sector_score >= 0.85:\n","                match_details.append(\"âœ“ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø·Ø§Ø¹\")\n","            elif sector_score >= 0.7:\n","                match_details.append(\"Ù‚Ø·Ø§Ø¹ Ù…Ø·Ø§Ø¨Ù‚\")\n","\n","        results.append({\n","            \"Ø§Ø³Ù…_Ø§Ù„Ù…Ù†ØªØ¬\": row_product if row_product else \"ØºÙŠØ± Ù…Ø­Ø¯Ø¯\",\n","            \"Ø§Ù„Ù‚Ø·Ø§Ø¹\": row_sector if row_sector else \"ØºÙŠØ± Ù…Ø­Ø¯Ø¯\",\n","            \"Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª\": classifications,\n","            \"Ø§Ù„Ø«Ù‚Ø©\": total_confidence,\n","            \"Ø¯Ø±Ø¬Ø©_Ø§Ù„Ù…Ù†ØªØ¬\": product_score,\n","            \"Ø¯Ø±Ø¬Ø©_Ø§Ù„Ù‚Ø·Ø§Ø¹\": sector_score,\n","            \"Ù†ÙˆØ¹_Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©\": \" | \".join(match_details) if match_details else \"Ø¹Ø§Ù…\",\n","            \"index\": index\n","        })\n","\n","    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ø«Ù‚Ø©\n","    results.sort(key=lambda x: x[\"Ø§Ù„Ø«Ù‚Ø©\"], reverse=True)\n","\n","    if results:\n","        search_info = []\n","        if product_name:\n","            search_info.append(f\"ğŸ“¦ Ø§Ù„Ù…Ù†ØªØ¬: '{product_name}'\")\n","        if sector_name:\n","            search_info.append(f\"ğŸ¢ Ø§Ù„Ù‚Ø·Ø§Ø¹: '{sector_name}'\")\n","\n","        return {\n","            \"found\": True,\n","            \"query\": \" | \".join(search_info),\n","            \"count\": len(results),\n","            \"results\": results[:top_n]\n","        }\n","    else:\n","        return {\n","            \"found\": False,\n","            \"query\": f\"{product_name} - {sector_name}\",\n","            \"message\": f\"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ØªØ¬/Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© (Ø«Ù‚Ø© Ø£Ù‚Ù„ Ù…Ù† {min_confidence:.0%})\"\n","        }\n","\n","# ===============================\n","# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n","# ===============================\n","def display_results(result, show_all_classifications=True, show_scores=True):\n","    \"\"\"Ø¹Ø±Ø¶ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù…\"\"\"\n","\n","    print(\"\\n\" + \"=\" * 100)\n","    print(f\"ğŸ” Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«: {result.get('query', 'N/A')}\")\n","    print(\"=\" * 100)\n","\n","    if not result[\"found\"]:\n","        print(f\"\\n{result.get('message', 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬')}\\n\")\n","        return\n","\n","    print(f\"\\nâœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {result['count']} Ù†ØªÙŠØ¬Ø©:\\n\")\n","\n","    for i, item in enumerate(result[\"results\"], 1):\n","        confidence = item['Ø§Ù„Ø«Ù‚Ø©']\n","\n","        # ØªØ­Ø¯ÙŠØ¯ Ø£ÙŠÙ‚ÙˆÙ†Ø© Ø§Ù„Ø«Ù‚Ø©\n","        if confidence >= 0.9:\n","            icon = \"ğŸŸ¢\"\n","            level = \"Ù…Ù…ØªØ§Ø²\"\n","        elif confidence >= 0.75:\n","            icon = \"ğŸŸ¡\"\n","            level = \"Ø¬ÙŠØ¯\"\n","        elif confidence >= 0.6:\n","            icon = \"ğŸŸ \"\n","            level = \"Ù…ØªÙˆØ³Ø·\"\n","        else:\n","            icon = \"ğŸ”´\"\n","            level = \"Ø¶Ø¹ÙŠÙ\"\n","\n","        print(f\"{i:3d}. {icon} [{level}] Ø§Ù„Ø«Ù‚Ø©: {confidence:.3f}\")\n","        print(f\"     ğŸ“¦ Ø§Ù„Ù…Ù†ØªØ¬: {item['Ø§Ø³Ù…_Ø§Ù„Ù…Ù†ØªØ¬']}\")\n","        print(f\"     ğŸ¢ Ø§Ù„Ù‚Ø·Ø§Ø¹: {item['Ø§Ù„Ù‚Ø·Ø§Ø¹']}\")\n","        print(f\"     ğŸ¯ {item['Ù†ÙˆØ¹_Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©']}\")\n","\n","        if show_scores:\n","            print(f\"     ğŸ“Š Ø¯Ø±Ø¬Ø© Ø§Ù„Ù…Ù†ØªØ¬: {item['Ø¯Ø±Ø¬Ø©_Ø§Ù„Ù…Ù†ØªØ¬']:.3f} | Ø¯Ø±Ø¬Ø© Ø§Ù„Ù‚Ø·Ø§Ø¹: {item['Ø¯Ø±Ø¬Ø©_Ø§Ù„Ù‚Ø·Ø§Ø¹']:.3f}\")\n","\n","        if item['Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª'] and show_all_classifications:\n","            print(f\"     ğŸ“‹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª:\")\n","            for class_name, class_value in list(item['Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª'].items())[:5]:\n","                short_name = class_name[:40] + \"...\" if len(class_name) > 40 else class_name\n","                short_value = class_value[:55] + \"...\" if len(class_value) > 55 else class_value\n","                print(f\"        â€¢ {short_name}: {short_value}\")\n","\n","            if len(item['Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª']) > 5:\n","                print(f\"        ... Ùˆ {len(item['Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª']) - 5} ØªØµÙ†ÙŠÙ Ø¢Ø®Ø±\")\n","\n","        print()\n","\n","        if i % 5 == 0 and i < len(result[\"results\"]):\n","            print(\"   \" + \"-\" * 90)\n","\n","    print(\"=\" * 100 + \"\\n\")\n","\n","def quick_display(result):\n","    \"\"\"Ø¹Ø±Ø¶ Ù…Ø®ØªØµØ± Ù„Ù„Ù†ØªØ§Ø¦Ø¬\"\"\"\n","\n","    print(f\"\\nğŸ” {result.get('query', 'Ø¨Ø­Ø«')}\")\n","    print(\"â”€\" * 80)\n","\n","    if not result[\"found\"]:\n","        print(f\"âŒ {result.get('message', 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬')}\\n\")\n","        return\n","\n","    print(f\"âœ… {result['count']} Ù†ØªÙŠØ¬Ø©\\n\")\n","\n","    for i, item in enumerate(result[\"results\"][:15], 1):\n","        if item['Ø§Ù„Ø«Ù‚Ø©'] >= 0.9:\n","            conf_icon = \"ğŸŸ¢\"\n","        elif item['Ø§Ù„Ø«Ù‚Ø©'] >= 0.75:\n","            conf_icon = \"ğŸŸ¡\"\n","        else:\n","            conf_icon = \"ğŸŸ \"\n","\n","        print(f\"{i:2d}. {conf_icon} {item['Ø§Ø³Ù…_Ø§Ù„Ù…Ù†ØªØ¬'][:65]}\")\n","        print(f\"    Ø§Ù„Ù‚Ø·Ø§Ø¹: {item['Ø§Ù„Ù‚Ø·Ø§Ø¹'][:40]} | Ø«Ù‚Ø©: {item['Ø§Ù„Ø«Ù‚Ø©']:.3f}\")\n","        print(f\"    {item['Ù†ÙˆØ¹_Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©']}\")\n","        print()\n","\n","    if result['count'] > 15:\n","        print(f\"... Ùˆ {result['count'] - 15} Ù†ØªÙŠØ¬Ø© Ø£Ø®Ø±Ù‰\\n\")\n","\n","# ===============================\n","# Ø¯Ø§Ù„Ø© Ø¨Ø­Ø« Ø³Ø±ÙŠØ¹ âš¡\n","# ===============================\n","def s(product=\"\", sector=\"\", conf=0.7, n=20, show_full=False):\n","    \"\"\"\n","    Ø¨Ø­Ø« Ø³Ø±ÙŠØ¹ Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n","\n","    Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:\n","    s(\"Ø­Ù„ÙŠØ¨ ÙƒØ§Ù…Ù„ Ø§Ù„Ø¯Ø³Ù…\")              # Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© ÙƒØ§Ù…Ù„Ø©\n","    s(\"Ø£Ø³Ù…Ù†Øª Ø¨ÙˆØ±ØªÙ„Ø§Ù†Ø¯\", \"Ù…ÙˆØ§Ø¯ Ø¨Ù†Ø§Ø¡\")  # Ù…Ù†ØªØ¬ + Ù‚Ø·Ø§Ø¹\n","    s(sector=\"Ù…Ù†ØªØ¬Ø§Øª Ø£Ù„Ø¨Ø§Ù†\")          # Ù‚Ø·Ø§Ø¹ ÙÙ‚Ø·\n","    s(\"Ø­Ù„ÙŠØ¨ ÙƒØ§Ù…Ù„ Ø§Ù„Ø¯Ø³Ù…\", conf=0.85)  # Ø±ÙØ¹ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø«Ù‚Ø©\n","    s(\"Ø­Ù„ÙŠØ¨\", n=30)                   # 30 Ù†ØªÙŠØ¬Ø©\n","    s(\"Ø­Ù„ÙŠØ¨\", show_full=True)        # Ø¹Ø±Ø¶ ÙƒØ§Ù…Ù„ Ø¨Ø§Ù„ØªÙØ§ØµÙŠÙ„\n","\n","    Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ù„Ø¨Ø­Ø« ÙŠØ¯ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù…Ù„Ø© ÙƒÙ„Ù‡Ø§ Ù…Ø¹ Ø¨Ø¹Ø¶ØŒ Ù…Ø´ ÙƒÙ„ ÙƒÙ„Ù…Ø© Ù„ÙˆØ­Ø¯Ù‡Ø§!\n","    \"\"\"\n","    result = enhanced_search(\n","        product_name=product,\n","        sector_name=sector,\n","        min_confidence=conf,\n","        top_n=n\n","    )\n","\n","    if show_full:\n","        display_results(result, show_all_classifications=True, show_scores=True)\n","    else:\n","        quick_display(result)\n","\n","    return result\n","\n","# ===============================\n","# Ø£Ù…Ø«Ù„Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…\n","# ===============================\n","def test_search():\n","    \"\"\"Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\"\"\"\n","\n","    print(\"\\n\" + \"ğŸ§ª\" * 40)\n","    print(\"Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\")\n","    print(\"ğŸ§ª\" * 40 + \"\\n\")\n","\n","    # Ù…Ø«Ø§Ù„ 1: Ø¨Ø­Ø« Ø¨Ø¬Ù…Ù„Ø© ÙƒØ§Ù…Ù„Ø©\n","    print(\"1ï¸âƒ£ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†: 'Ø­Ù„ÙŠØ¨ ÙƒØ§Ù…Ù„ Ø§Ù„Ø¯Ø³Ù…' (Ø¬Ù…Ù„Ø© ÙƒØ§Ù…Ù„Ø©)\")\n","    result1 = enhanced_search(product_name=\"Ø­Ù„ÙŠØ¨ ÙƒØ§Ù…Ù„ Ø§Ù„Ø¯Ø³Ù…\")\n","    quick_display(result1)\n","\n","    # Ù…Ø«Ø§Ù„ 2: Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù‚Ø·Ø§Ø¹\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"2ï¸âƒ£ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù‚Ø·Ø§Ø¹: 'Ù…Ù†ØªØ¬Ø§Øª Ø£Ù„Ø¨Ø§Ù†'\")\n","    result2 = enhanced_search(sector_name=\"Ù…Ù†ØªØ¬Ø§Øª Ø£Ù„Ø¨Ø§Ù†\")\n","    quick_display(result2)\n","\n","    # Ù…Ø«Ø§Ù„ 3: Ø¨Ø­Ø« Ù…Ø±ÙƒØ¨\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"3ï¸âƒ£ Ø¨Ø­Ø« Ù…Ø±ÙƒØ¨: 'Ø£Ø³Ù…Ù†Øª Ø¨ÙˆØ±ØªÙ„Ø§Ù†Ø¯' ÙÙŠ 'Ù…ÙˆØ§Ø¯ Ø¨Ù†Ø§Ø¡'\")\n","    result3 = enhanced_search(\n","        product_name=\"Ø£Ø³Ù…Ù†Øª Ø¨ÙˆØ±ØªÙ„Ø§Ù†Ø¯\",\n","        sector_name=\"Ù…ÙˆØ§Ø¯ Ø¨Ù†Ø§Ø¡\",\n","        min_confidence=0.7\n","    )\n","    display_results(result3, show_all_classifications=True, show_scores=True)\n","\n","# ===============================\n","# Ø§Ù„ØªØ´ØºÙŠÙ„\n","# ===============================\n","if __name__ == \"__main__\":\n","    print(\"\\nğŸ“Š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù:\")\n","    print(f\"   â€¢ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ: {len(df)}\")\n","    print(f\"   â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {len(df.columns)}\")\n","\n","    print(\"\\nğŸ’¡ Ù…Ù„Ø§Ø­Ø¸Ø© Ù…Ù‡Ù…Ø©:\")\n","    print(\"   Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¢Ù† ÙŠØ¯ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù…Ù„Ø© ÙƒØ§Ù…Ù„Ø©ØŒ Ù…Ø´ ÙƒÙ„ ÙƒÙ„Ù…Ø© Ù„ÙˆØ­Ø¯Ù‡Ø§!\")\n","    print(\"   Ù…Ø«Ø§Ù„: 'Ø­Ù„ÙŠØ¨ ÙƒØ§Ù…Ù„ Ø§Ù„Ø¯Ø³Ù…' â† ÙŠØ¯ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù…Ù„Ø© Ø¯ÙŠ ÙƒÙ„Ù‡Ø§\\n\")\n","\n","    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª\n","    test_search()\n","\n","    print(\"\\n\" + \"ğŸ’¡\" * 40)\n","    print(\"âœ… Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø±ÙŠØ¹:\")\n","    print(\"   s('Ø­Ù„ÙŠØ¨ ÙƒØ§Ù…Ù„ Ø§Ù„Ø¯Ø³Ù…')       # Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¬Ù…Ù„Ø©\")\n","    print(\"   s('Ø£Ø³Ù…Ù†Øª', 'Ø¨Ù†Ø§Ø¡')         # Ù…Ù†ØªØ¬ + Ù‚Ø·Ø§Ø¹\")\n","    print(\"   s('Ø­Ù„ÙŠØ¨', conf=0.85)       # Ø±ÙØ¹ Ø§Ù„Ø«Ù‚Ø©\")\n","    print(\"ğŸ’¡\" * 40)"],"metadata":{"id":"nP9YMdChqjOB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install flask pandas fuzzywuzzy"],"metadata":{"id":"fLiSlepryY9g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","from difflib import SequenceMatcher\n","from flask import Flask, request, jsonify\n","\n","# ===============================\n","# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n","# ===============================\n","\n","# Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ Flask\n","app = Flask(__name__)\n","\n","# Ø§Ù„Ù…Ø³Ø§Ø± Ø¥Ù„Ù‰ Ù…Ù„Ù CSV (ÙŠØ±Ø¬Ù‰ ØªØ¹Ø¯ÙŠÙ„Ù‡ Ø­Ø³Ø¨ Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ´ØºÙŠÙ„)\n","# Ù…Ù„Ø§Ø­Ø¸Ø©: ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù…Ù„Ù Ù…ØªØ§Ø­Ø§Ù‹ Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù€ API Ø¹Ù†Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„\n","csv_path = \"/content/drive/MyDrive/AutoModelForSequenceClassification/updates.csv\"\n","\n","try:\n","    df = pd.read_csv(csv_path, encoding='utf-8')\n","    print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­\")\n","except UnicodeDecodeError:\n","    try:\n","        df = pd.read_csv(csv_path, encoding='utf-8-sig')\n","        print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­ (utf-8-sig)\")\n","    except:\n","        try:\n","            df = pd.read_csv(csv_path, encoding='cp1256')\n","            print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­ (cp1256)\")\n","        except Exception as e:\n","            print(f\"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {str(e)}\")\n","            # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù„Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¨Ø­Ø« ÙÙŠÙ‡Ø§\n","            raise RuntimeError(\"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\") from e\n","\n","print(f\"\\nâœ… Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª: {len(df)}\")\n","print(f\"âœ… Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {len(df.columns)}\")\n","\n","# ===============================\n","# Ø¯Ø§Ù„Ø© ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Ù…ÙÙ†Ø³ÙˆØ®Ø© Ù…Ù† ÙƒÙˆØ¯Ùƒ)\n","# ===============================\n","def normalize_arabic(text):\n","    \"\"\"ØªÙ†Ø¸ÙŠÙ ÙˆØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\"\"\"\n","    if pd.isna(text):\n","        return \"\"\n","\n","    text = str(text).strip()\n","    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)\n","    text = re.sub(r'Ø©', 'Ù‡', text)\n","    text = re.sub(r'Ù‰', 'ÙŠ', text)\n","    text = re.sub(r'Ø¦', 'Ø¡', text)\n","    text = re.sub(r'[^\\w\\s]', ' ', text)\n","    text = ' '.join(text.split())\n","\n","    return text.lower()\n","\n","# ===============================\n","# Ø¯Ø§Ù„Ø© Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¬Ù…Ù„Ø© (Ù…ÙÙ†Ø³ÙˆØ®Ø© Ù…Ù† ÙƒÙˆØ¯Ùƒ)\n","# ===============================\n","def phrase_search_score(search_phrase, target_text):\n","    \"\"\"\n","    Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© - Ø§Ù„Ø¬Ù…Ù„Ø© Ù„Ø§Ø²Ù… ØªÙƒÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯Ø© ÙƒÙ„Ù‡Ø§ Ù…Ø¹ Ø¨Ø¹Ø¶\n","    ... (Ø¨Ø§Ù‚ÙŠ Ø§Ù„ÙƒÙˆØ¯ ÙƒÙ…Ø§ Ù‡Ùˆ)\n","    \"\"\"\n","    if not search_phrase or not target_text:\n","        return 0.0\n","\n","    search_clean = normalize_arabic(search_phrase)\n","    target_clean = normalize_arabic(target_text)\n","\n","    # 1. Ø§Ù„Ø¬Ù…Ù„Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„Ø¶Ø¨Ø·\n","    if search_clean == target_clean:\n","        return 1.0\n","\n","    # 2. Ø§Ù„Ø¬Ù…Ù„Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙƒØ¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù†Øµ\n","    if search_clean in target_clean:\n","        phrase_length = len(search_clean)\n","        target_length = len(target_clean)\n","        coverage = phrase_length / target_length\n","        base_score = 0.85\n","        bonus = coverage * 0.15\n","        return min(base_score + bonus, 1.0)\n","\n","    # 3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¹Ø§Ù„ÙŠ Ù„Ù„Ø¬Ù…Ù„Ø©\n","    similarity = SequenceMatcher(None, search_clean, target_clean).ratio()\n","    if similarity >= 0.9:\n","        return similarity\n","\n","    # 4. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¬Ù…Ù„Ø© Ø¨ØªØ±ØªÙŠØ¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª\n","    search_words = search_clean.split()\n","    target_words = target_clean.split()\n","\n","    if len(search_words) > 1:\n","        search_index = 0\n","        for target_word in target_words:\n","            if search_index < len(search_words):\n","                if search_words[search_index] in target_word or target_word in search_words[search_index]:\n","                    search_index += 1\n","\n","        if search_index == len(search_words):\n","            match_ratio = len(search_words) / len(target_words)\n","            return 0.7 + (match_ratio * 0.2)\n","\n","    # 5. Ù„Ùˆ Ø§Ù„Ø¬Ù…Ù„Ø© Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯Ø© Ø®Ø§Ù„Øµ\n","    return 0.0\n","\n","# ===============================\n","# Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø³Ù‘Ù† (Ù…ÙÙ†Ø³ÙˆØ®Ø© Ù…Ù† ÙƒÙˆØ¯Ùƒ)\n","# ===============================\n","def enhanced_search(product_name=\"\", sector_name=\"\", min_confidence=0.7, top_n=30):\n","    \"\"\"\n","    Ø¨Ø­Ø« Ù…Ø­Ø³Ù‘Ù† - ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¬Ù…Ù„Ø© ÙƒØ§Ù…Ù„Ø© Ù…Ø´ ÙƒÙ„ ÙƒÙ„Ù…Ø© Ù„ÙˆØ­Ø¯Ù‡Ø§\n","    ... (Ø¨Ø§Ù‚ÙŠ Ø§Ù„ÙƒÙˆØ¯ ÙƒÙ…Ø§ Ù‡Ùˆ)\n","    \"\"\"\n","    global df # Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… DataFrame Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ ÙÙŠ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©\n","\n","    if not product_name and not sector_name:\n","        return {\n","            \"found\": False,\n","            \"message\": \"âš ï¸ ÙŠØ¬Ø¨ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬ Ø£Ùˆ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„\"\n","        }\n","\n","    results = []\n","\n","    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙƒÙ„ ØµÙ\n","    for index, row in df.iterrows():\n","        # ... (Ù…Ù†Ø·Ù‚ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹ ÙˆØ§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© ÙƒÙ…Ø§ Ù‡Ùˆ ÙÙŠ ÙƒÙˆØ¯Ùƒ)\n","        row_product = str(row.get('Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬', row.get('product_name', row.get('Ø§Ù„Ù…Ù†ØªØ¬', ''))))\n","        row_sector = str(row.get('Ø§Ù„Ù‚Ø·Ø§Ø¹', row.get('sector', row.get('Sector', ''))))\n","\n","        if row_product == \"nan\":\n","            row_product = \"\"\n","        if row_sector == \"nan\":\n","            row_sector = \"\"\n","\n","        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…Ù†ØªØ¬\n","        product_score = 0.0\n","        if product_name:\n","            product_score = phrase_search_score(product_name, row_product)\n","        else:\n","            product_score = 0.5\n","\n","        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù‚Ø·Ø§Ø¹\n","        sector_score = 0.0\n","        if sector_name:\n","            sector_score = phrase_search_score(sector_name, row_sector)\n","        else:\n","            sector_score = 0.5\n","\n","        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© (ÙˆØ²Ù† Ø£ÙƒØ¨Ø± Ù„Ù„Ù…Ù†ØªØ¬)\n","        total_confidence = (product_score * 0.7) + (sector_score * 0.3)\n","\n","        # ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø«Ù‚Ø©\n","        if total_confidence < min_confidence:\n","            continue\n","\n","        # Ø¬Ù…Ø¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª\n","        classifications = {}\n","        for col in df.columns:\n","            if col not in ['Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬', 'Ø§Ù„Ù‚Ø·Ø§Ø¹', 'product_name', 'sector', 'Sector', 'Ø§Ù„Ù…Ù†ØªØ¬']:\n","                value = str(row.get(col, \"\"))\n","                if value and value != \"nan\" and value.strip():\n","                    classifications[col] = value\n","\n","        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØªØ·Ø§Ø¨Ù‚\n","        match_details = []\n","        if product_name and product_score > 0.5:\n","            if product_score >= 0.95:\n","                match_details.append(\"âœ“ Ù…Ø·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ø§Ù‹\")\n","            elif product_score >= 0.85:\n","                match_details.append(\"âœ“ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù…Ù„Ø© ÙƒØ§Ù…Ù„Ø©\")\n","            elif product_score >= 0.7:\n","                match_details.append(\"ØªØ·Ø§Ø¨Ù‚ Ù‚ÙˆÙŠ Ù„Ù„Ø¬Ù…Ù„Ø©\")\n","            else:\n","                match_details.append(\"ØªØ·Ø§Ø¨Ù‚ Ø¬Ø²Ø¦ÙŠ\")\n","\n","        if sector_name and sector_score > 0.5:\n","            if sector_score >= 0.95:\n","                match_details.append(\"âœ“ Ù‚Ø·Ø§Ø¹ Ù…Ø·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ø§Ù‹\")\n","            elif sector_score >= 0.85:\n","                match_details.append(\"âœ“ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø·Ø§Ø¹\")\n","            elif sector_score >= 0.7:\n","                match_details.append(\"Ù‚Ø·Ø§Ø¹ Ù…Ø·Ø§Ø¨Ù‚\")\n","\n","\n","        results.append({\n","            \"Ø§Ø³Ù…_Ø§Ù„Ù…Ù†ØªØ¬\": row_product if row_product else \"ØºÙŠØ± Ù…Ø­Ø¯Ø¯\",\n","            \"Ø§Ù„Ù‚Ø·Ø§Ø¹\": row_sector if row_sector else \"ØºÙŠØ± Ù…Ø­Ø¯Ø¯\",\n","            \"Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª\": classifications,\n","            \"Ø§Ù„Ø«Ù‚Ø©\": round(total_confidence, 4), # ØªÙ‚Ø±ÙŠØ¨ Ù„Ø£Ø±Ø¨Ø¹ Ù…Ù†Ø§Ø²Ù„\n","            \"Ø¯Ø±Ø¬Ø©_Ø§Ù„Ù…Ù†ØªØ¬\": round(product_score, 4),\n","            \"Ø¯Ø±Ø¬Ø©_Ø§Ù„Ù‚Ø·Ø§Ø¹\": round(sector_score, 4),\n","            \"Ù†ÙˆØ¹_Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©\": \" | \".join(match_details) if match_details else \"Ø¹Ø§Ù…\",\n","            \"index\": int(index) # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† index Ù‡Ùˆ Ø¹Ø¯Ø¯ ØµØ­ÙŠØ­ Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ³Ù„Ø³Ù„ ÙÙŠ JSON\n","        })\n","\n","    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ø«Ù‚Ø©\n","    results.sort(key=lambda x: x[\"Ø§Ù„Ø«Ù‚Ø©\"], reverse=True)\n","\n","    if results:\n","        search_info = []\n","        if product_name:\n","            search_info.append(f\"ğŸ“¦ Ø§Ù„Ù…Ù†ØªØ¬: '{product_name}'\")\n","        if sector_name:\n","            search_info.append(f\"ğŸ¢ Ø§Ù„Ù‚Ø·Ø§Ø¹: '{sector_name}'\")\n","\n","        return {\n","            \"found\": True,\n","            \"query\": \" | \".join(search_info),\n","            \"count\": len(results),\n","            \"results\": results[:top_n]\n","        }\n","    else:\n","        return {\n","            \"found\": False,\n","            \"query\": f\"{product_name} - {sector_name}\",\n","            \"message\": f\"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ØªØ¬/Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© (Ø«Ù‚Ø© Ø£Ù‚Ù„ Ù…Ù† {min_confidence:.0%})\"\n","        }\n","\n","\n","# ===============================\n","# ØªØ¹Ø±ÙŠÙ Ù…Ø³Ø§Ø± Ø§Ù„Ù€ API (API Endpoint)\n","# ===============================\n","@app.route('/search', methods=['GET'])\n","def search_api():\n","    \"\"\"\n","    ÙˆØ§Ø¬Ù‡Ø© API Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù†ØªØ¬ ÙˆØ§Ù„Ù‚Ø·Ø§Ø¹.\n","    Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:\n","    - product_name (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)\n","    - sector_name (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)\n","    - min_confidence (Ø§Ø®ØªÙŠØ§Ø±ÙŠØŒ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© 0.7)\n","    - top_n (Ø§Ø®ØªÙŠØ§Ø±ÙŠØŒ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© 30)\n","    \"\"\"\n","\n","    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ù† Ø·Ù„Ø¨ GET\n","    product = request.args.get('product_name', default=\"\", type=str)\n","    sector = request.args.get('sector_name', default=\"\", type=str)\n","\n","    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø«Ù‚Ø© ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…\n","    try:\n","        min_conf = float(request.args.get('min_confidence', default=0.7))\n","        top_results = int(request.args.get('top_n', default=30))\n","    except ValueError:\n","        return jsonify({\n","            \"error\": \"min_confidence Ùˆ top_n ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ†Ø§ Ø£Ø±Ù‚Ø§Ù…Ø§Ù‹ ØµØ­ÙŠØ­Ø©/Ø¹Ø´Ø±ÙŠØ©\"\n","        }), 400\n","\n","    # ØªÙ†ÙÙŠØ° Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø³Ù‘Ù†\n","    search_result = enhanced_search(\n","        product_name=product,\n","        sector_name=sector,\n","        min_confidence=min_conf,\n","        top_n=top_results\n","    )\n","\n","    # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON\n","    return jsonify(search_result)\n","\n","# ===============================\n","# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚\n","# ===============================\n"],"metadata":{"id":"MOt8b0YPyamY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xdW05lA_yrYt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pyngrok --quiet\n","\n","from pyngrok import ngrok\n","from threading import Thread\n","import getpass\n","\n","# 1) Auth token (only once per runtime)\n","print(\"Enter your Ngrok Auth Token:\")\n","ngrok.set_auth_token(getpass.getpass())\n","\n","# 2) Function to run Flask in background\n","def run_flask():\n","    print(\"\\n--- ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… Flask ---\")\n","    print(\"ğŸš€ API Ø¬Ø§Ù‡Ø²Ø© Ø¹Ù„Ù‰: http://127.0.0.1:5000/search\")\n","    # important: host=0.0.0.0 so ngrok can see it\n","    app.run(host=\"0.0.0.0\", port=5000, debug=False, use_reloader=False)\n","\n","# 3) Start Flask in a background thread (non-blocking)\n","flask_thread = Thread(target=run_flask)\n","flask_thread.daemon = True\n","flask_thread.start()\n","\n","# 4) Open ngrok tunnel to port 5000\n","public_url = ngrok.connect(5000).public_url\n","print(f\"\\nâœ… Ngrok Tunnel ready!\")\n","print(f\"ğŸ”— Public API URL: {public_url}/search\")\n"],"metadata":{"id":"UXI6iJfsy1yr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ø¨Ø­Ø« Ø¨Ø¬Ù…Ù„Ø© ÙƒØ§Ù…Ù„Ø©\n","s(\" ÙƒØ§Ù…Ù„ Ø§Ù„Ø¯Ø³Ù…\")"],"metadata":{"id":"8R2QiysNqzNd"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}